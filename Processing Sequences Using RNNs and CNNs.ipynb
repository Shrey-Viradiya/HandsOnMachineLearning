{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Processing Sequences Using RNNs and CNNs","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\nimport matplotlib as mpl\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Suppose you are studying the number of active users per hour on your website, or the daily temperature in your city, or your company’s financial health, measured quarterly using multiple metrics. In all these cases, the data will be a sequence of one or more values per time step. This is called a time series. In the first two examples there is a single value per time step, so these are univariate time series, while in the financial example there are multiple values per time step (e.g., the company’s revenue, debt, and so on), so it is a multivariate time series. A typical task is to predict future values, which is called forecasting. Another common task is to fill in the blanks: to predict (or rather “postdict”) missing values from the past. This is called imputation.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Generating Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_time_series(batch_size, n_steps):\n    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n    time = np.linspace(0, 1, n_steps)\n    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1\n    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n    return series[..., np.newaxis].astype(np.float32)\n#     return series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = 50\nseries = generate_time_series(10000, n_steps + 1)\nX_train, y_train = series[:7000, :n_steps], series[:7000, -1]\nX_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\nX_test, y_test = series[9000:, :n_steps], series[9000:, -1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curves(loss, val_loss):\n    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.-\", label=\"Training loss\")\n    plt.plot(np.arange(len(val_loss)) + 1, val_loss, \"r.-\", label=\"Validation loss\")\n    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n    plt.legend()\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.grid(True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_series(series, y=None, y_pred=None, x_label=\"$t$\", y_label=\"$x(t)$\"):\n    plt.plot(series, \".-\")\n    if y is not None:\n        plt.plot(n_steps, y, \"bx\", markersize=10)\n    if y_pred is not None:\n        plt.plot(n_steps, y_pred, \"ro\")\n    plt.grid(True)\n    if x_label:\n        plt.xlabel(x_label)\n    if y_label:\n        plt.ylabel(y_label, rotation=0)\n    plt.hlines(0, 0, 100, linewidth=1)\n    plt.axis([0, n_steps + 1, -1, 1])\n\nfig, axes = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(12, 4))\nfor col in range(3):\n    plt.sca(axes[col])\n    plot_series(X_valid[col, :, 0], y_valid[col, 0],\n                y_label=(\"$x(t)$\" if col==0 else None))\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Computing Some Baselines","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before we start using RNNs, it is often a good idea to have a few baseline metrics, or else we may end up thinking our model works great when in fact it is doing worse than basic models. For example, the simplest approach is to predict the last value in each series. This is called naive forecasting, and it is sometimes surprisingly difficult to outperform.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nNaive predictions (just predict the last observed value):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = X_valid[:, -1]\nnp.mean(keras.losses.mean_squared_error(y_valid, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another simple approach is to use a fully connected network. Since it expects a flat list of features for each input, we need to add a Flatten layer. Let’s just use a simple Linear Regression model so that each prediction will be a linear combination of the values in the time series:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.Sequential([\n    keras.layers.Flatten(input_shape = [50,1]),\n    keras.layers.Dense(1)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=\"mse\", optimizer=\"adam\")\nhistory = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing a Simple RNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.Sequential([\n  keras.layers.SimpleRNN(1, input_shape=[None, 1])\n])\n\nmodel.compile(loss=\"mse\", optimizer=\"adam\")\nhistory = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.Sequential([\n    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n    keras.layers.SimpleRNN(20, return_sequences=True),\n    keras.layers.SimpleRNN(1)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=\"mse\", optimizer=\"adam\")\nhistory = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.Sequential([\n    keras.layers.SimpleRNN(20, return_sequences = True, input_shape=[None, 1]),\n    keras.layers.SimpleRNN(20),\n    keras.layers.Dense(1)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=\"mse\", optimizer=\"adam\")\nhistory = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Forecasting Several Steps Ahead","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(259)\n\nseries = generate_time_series(1, n_steps + 10)\nX_new, Y_new = series[:, :n_steps], series[:, n_steps:]\nX = X_new\nfor step_ahead in range(10):\n    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n    X = np.concatenate([X, y_pred_one], axis=1)\n\nY_pred = X[:, n_steps:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_multiple_forecasts(X, Y, Y_pred):\n    n_steps = X.shape[1]\n    ahead = Y.shape[1]\n    plot_series(X[0, :, 0])\n    plt.plot(np.arange(n_steps, n_steps + ahead), Y[0, :, 0], \"ro-\", label=\"Actual\")\n    plt.plot(np.arange(n_steps, n_steps + ahead), Y_pred[0, :, 0], \"bx-\", label=\"Forecast\", markersize=10)\n    plt.axis([0, n_steps + ahead, -1, 1])\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_multiple_forecasts(X_new, Y_new, Y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's use this model to predict the next 10 values. We first need to regenerate the sequences with 9 more time steps.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"series = generate_time_series(10000, n_steps + 10)\nX_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\nX_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\nX_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.Sequential([\n    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n    keras.layers.SimpleRNN(20),\n    keras.layers.Dense(10)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=\"mse\", optimizer=\"adam\")\nhistory = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model.predict(X_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(259)\n\nseries = generate_time_series(1, 50 + 10)\nX_new, Y_new = series[:, :50, :], series[:, -10:, :]\nY_pred = model.predict(X_new)[..., np.newaxis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_multiple_forecasts(X_new, Y_new, Y_pred)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nNow let's create an RNN that predicts the next 10 steps at each time step. That is, instead of just forecasting time steps 50 to 59 based on time steps 0 to 49, it will forecast time steps 1 to 10 at time step 0, then time steps 2 to 11 at time step 1, and so on, and finally it will forecast time steps 50 to 59 at the last time step. Notice that the model is causal: when it makes predictions at any time step, it can only see past time steps.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = np.empty((10000, n_steps, 10))\nfor step_ahead in range(1, 10 + 1):\n    Y[:, :, step_ahead - 1] = series[:, step_ahead : step_ahead + n_steps, 0]\nY_train = Y[:7000]\nY_valid = Y[7000:9000]\nY_test  = Y[9000:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It may be surprising that the targets will contain values that appear in the inputs (there is a lot of overlap between X_train and Y_train). Isn’t that cheating? Fortunately, not at all: at each time step, the model only knows about past time steps, so it cannot look ahead. It is said to be a causal model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To turn the model into a sequence-to-sequence model, we must set return_sequences=True in all recurrent layers (even the last one), and we must apply the output Dense layer at every time step.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.Sequential([\n    keras.layers.SimpleRNN(20, return_sequences = True, input_shape=[None, 1]),\n    keras.layers.SimpleRNN(20, return_sequences = True),\n    keras.layers.TimeDistributed(keras.layers.Dense(10))\n])\n\ndef last_time_step_mse(Y_true, Y_pred):\n    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss = 'mse', optimizer = 'nadam', metrics = [last_time_step_mse])\nhistory = model.fit(X_train, Y_train, validation_data = (X_valid, Y_valid), epochs=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(258)\n\nseries = generate_time_series(1, 50 + 10)\nX_new, Y_new = series[:, :50, :], series[:, 50:, :]\nY_pred = model.predict(X_new)[:, -1][..., np.newaxis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_multiple_forecasts(X_new, Y_new, Y_pred)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}